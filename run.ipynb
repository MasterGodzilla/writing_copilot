{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOGETHER_API_KEY\"] = \"79ac5aa7321cd3c42d6fea16247b61b13b27d39c661823139fc669116c13fcf7\"\n",
    "from together import Together\n",
    "from typing import List, Dict\n",
    "\n",
    "def completion(prompt: str, \n",
    "               model: str = \"Qwen/Qwen1.5-110B-Chat\", \n",
    "               max_tokens: int = 1000,\n",
    "               temperature: float = 0.6,\n",
    "               stop: List[str] = [\"</s>\"],\n",
    "               logit_bias: Dict[str, float] = {}):\n",
    "    client = Together(api_key=os.environ.get(\"TOGETHER_API_KEY\"))\n",
    "    response = client.completions.create(\n",
    "        model=model,\n",
    "        prompt=prompt,\n",
    "        max_tokens=max_tokens,\n",
    "        temperature=temperature,\n",
    "        stop=stop,\n",
    "        logit_bias=logit_bias\n",
    "    )\n",
    "    return response.choices[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openai\n",
      "  Downloading openai-1.30.3-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting anyio<5,>=3.5.0 (from openai)\n",
      "  Downloading anyio-4.4.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting distro<2,>=1.7.0 (from openai)\n",
      "  Downloading distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting httpx<1,>=0.23.0 (from openai)\n",
      "  Downloading httpx-0.27.0-py3-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /Users/ryan_sun/miniconda3/envs/together/lib/python3.12/site-packages (from openai) (2.7.0)\n",
      "Collecting sniffio (from openai)\n",
      "  Downloading sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: tqdm>4 in /Users/ryan_sun/miniconda3/envs/together/lib/python3.12/site-packages (from openai) (4.66.2)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in /Users/ryan_sun/miniconda3/envs/together/lib/python3.12/site-packages (from openai) (4.11.0)\n",
      "Requirement already satisfied: idna>=2.8 in /Users/ryan_sun/miniconda3/envs/together/lib/python3.12/site-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
      "Requirement already satisfied: certifi in /Users/ryan_sun/miniconda3/envs/together/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai) (2024.2.2)\n",
      "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
      "  Downloading httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
      "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Users/ryan_sun/miniconda3/envs/together/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.1 in /Users/ryan_sun/miniconda3/envs/together/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->openai) (2.18.1)\n",
      "Downloading openai-1.30.3-py3-none-any.whl (320 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m320.6/320.6 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading anyio-4.4.0-py3-none-any.whl (86 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Using cached h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "Installing collected packages: sniffio, h11, distro, httpcore, anyio, httpx, openai\n",
      "Successfully installed anyio-4.4.0 distro-1.9.0 h11-0.14.0 httpcore-1.0.5 httpx-0.27.0 openai-1.30.3 sniffio-1.3.1\n"
     ]
    }
   ],
   "source": [
    "!pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "client = openai.OpenAI(\n",
    "    api_key=os.environ.get(\"TOGETHER_API_KEY\"),\n",
    "    base_url=\"https://api.together.xyz/v1\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实充实'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = client.completions.create(\n",
    "    model=\"Qwen/Qwen1.5-32B-Chat\",\n",
    "    prompt=\"What is the capital of France?\",\n",
    "    max_tokens=1000,\n",
    "    temperature=0.6,\n",
    "    stop=[\"</s>\"],\n",
    "    logit_bias={108300: 100.0}\n",
    ")   \n",
    "\n",
    "response.choices[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Completions.create() got an unexpected keyword argument 'logit_bias'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m logit_bias \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m108300\u001b[39m: \u001b[38;5;241m100\u001b[39m}\n\u001b[0;32m----> 3\u001b[0m \u001b[43mcompletion\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mWhat is the meaning of life?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogit_bias\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 13\u001b[0m, in \u001b[0;36mcompletion\u001b[0;34m(prompt, model, max_tokens, temperature, stop, logit_bias)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompletion\u001b[39m(prompt: \u001b[38;5;28mstr\u001b[39m, \n\u001b[1;32m      7\u001b[0m                model: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQwen/Qwen1.5-110B-Chat\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[1;32m      8\u001b[0m                max_tokens: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1000\u001b[39m,\n\u001b[1;32m      9\u001b[0m                temperature: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.6\u001b[39m,\n\u001b[1;32m     10\u001b[0m                stop: List[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m</s>\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     11\u001b[0m                logit_bias: Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mfloat\u001b[39m] \u001b[38;5;241m=\u001b[39m {}):\n\u001b[1;32m     12\u001b[0m     client \u001b[38;5;241m=\u001b[39m Together(api_key\u001b[38;5;241m=\u001b[39mos\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTOGETHER_API_KEY\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m---> 13\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogit_bias\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtext\n",
      "\u001b[0;31mTypeError\u001b[0m: Completions.create() got an unexpected keyword argument 'logit_bias'"
     ]
    }
   ],
   "source": [
    "logit_bias = {108300: 100}\n",
    "\n",
    "completion(\"What is the meaning of life?\", logit_bias=logit_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /Users/ryan_sun/miniconda3/envs/together/lib/python3.12/site-packages (4.41.1)\n",
      "Requirement already satisfied: filelock in /Users/ryan_sun/miniconda3/envs/together/lib/python3.12/site-packages (from transformers) (3.13.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /Users/ryan_sun/miniconda3/envs/together/lib/python3.12/site-packages (from transformers) (0.23.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/ryan_sun/miniconda3/envs/together/lib/python3.12/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/ryan_sun/miniconda3/envs/together/lib/python3.12/site-packages (from transformers) (24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/ryan_sun/miniconda3/envs/together/lib/python3.12/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/ryan_sun/miniconda3/envs/together/lib/python3.12/site-packages (from transformers) (2024.5.15)\n",
      "Requirement already satisfied: requests in /Users/ryan_sun/miniconda3/envs/together/lib/python3.12/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /Users/ryan_sun/miniconda3/envs/together/lib/python3.12/site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /Users/ryan_sun/miniconda3/envs/together/lib/python3.12/site-packages (from transformers) (0.4.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/ryan_sun/miniconda3/envs/together/lib/python3.12/site-packages (from transformers) (4.66.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/ryan_sun/miniconda3/envs/together/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (2024.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/ryan_sun/miniconda3/envs/together/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/ryan_sun/miniconda3/envs/together/lib/python3.12/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/ryan_sun/miniconda3/envs/together/lib/python3.12/site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/ryan_sun/miniconda3/envs/together/lib/python3.12/site-packages (from requests->transformers) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/ryan_sun/miniconda3/envs/together/lib/python3.12/site-packages (from requests->transformers) (2024.2.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ryan_sun/miniconda3/envs/together/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen1.5-110B-Chat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabs = tokenizer.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langdetect in /Users/ryan_sun/miniconda3/envs/together/lib/python3.12/site-packages (1.0.9)\n",
      "Requirement already satisfied: six in /Users/ryan_sun/miniconda3/envs/together/lib/python3.12/site-packages (from langdetect) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install langdetect\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from langdetect import detect\n",
    "\n",
    "chinese_tokens = {}\n",
    "def detect_lang(idx: int):\n",
    "    text = tokenizer.decode([idx])\n",
    "    try:\n",
    "        if detect(text) == \"zh-cn\":\n",
    "            chinese_tokens[idx] = text\n",
    "            if idx % 100 == 0:\n",
    "                print(idx, text)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32100 错误\n",
      "70500 设计\n",
      "78900 。”\n",
      "\n",
      "\n",
      "81800 数量\n",
      "82700 产品\n",
      "99400 察\n",
      "99500 奖\n",
      "99600 活动\n",
      "99900 刚\n",
      "100000 也是\n",
      "100100 纷\n",
      "100200 卫生\n",
      "100300 纠\n",
      "100400 蓝\n",
      "100600 辖\n",
      "100700 详细\n",
      "101100 党员\n",
      "101300 进步\n",
      "101500 千万\n",
      "101700 税务\n",
      "101900 你们\n",
      "102000 宝宝\n",
      "102100 营养\n",
      "102200 也就是\n",
      "102400 药品\n",
      "102500 简称\n",
      "102800 少数\n",
      "103100 可谓\n",
      "103400 贝尔\n",
      "104300 年的\n",
      "104400 走进\n",
      "104800 实验室\n",
      "105100 意大\n",
      "105200 最为\n",
      "105300 一番\n",
      "105700 夫人\n",
      "105900 生产线\n",
      "106000 筒\n",
      "106100 的支持\n",
      "106400 跟我\n",
      "106600 缴纳\n",
      "107200 头部\n",
      "107300 仍是\n",
      "107400 过去的\n",
      "107700 水质\n",
      "107900 开辟\n",
      "108400 帮你\n",
      "108700 几位\n",
      "109200 户口\n",
      "109300 容器\n",
      "109500 千万不要\n",
      "109600 学费\n",
      "109800 绍兴\n",
      "110100 空军\n",
      "110600 这让\n",
      "110700 尤其是在\n",
      "111100 邓小\n",
      "111200 进驻\n",
      "111400 妥善\n",
      "111500 射手\n",
      "111900 本报记者\n",
      "112600 金币\n",
      "112800 谁能\n",
      "112900 图纸\n",
      "113000 责任人\n",
      "113100 在广州\n",
      "113300 大酒店\n",
      "113700 意义上的\n",
      "113800 运势\n",
      "113900 环境污染\n",
      "114000 老人家\n",
      "114200 机械设备\n",
      "114400 盗窃\n",
      "114500 时不\n",
      "114600 单价\n",
      "115100 没必要\n",
      "115200 年年底\n",
      "115400 住所\n",
      "115600 去掉\n",
      "115800 了很久\n",
      "116000 移送\n",
      "116100 会会长\n",
      "116400 版权归原\n",
      "116500 总队\n",
      "116600 学堂\n",
      "116800 双腿\n",
      "116900 先进技术\n",
      "117000 扰乱\n",
      "117200 设计理念\n",
      "117300 伺服\n",
      "117400 失调\n",
      "118300 承担责任\n",
      "118800 绘本\n",
      "118900 尤为重要\n"
     ]
    }
   ],
   "source": [
    "with ThreadPoolExecutor() as executor:\n",
    "    executor.map(detect_lang, range(tokenizer.vocab_size))\n",
    "\n",
    "# save chinese tokens\n",
    "with open(\"chinese_tokens.txt\", \"w\") as f:\n",
    "    for idx, text in chinese_tokens.items():\n",
    "        f.write(f\"{idx}\\t{text}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(74687, '。\\n\\n\\n\\n\\n\\n'),\n",
       " (18556, '。\\n\\n\\n\\n'),\n",
       " (53589, '。。\\n\\n'),\n",
       " (61993, \"。',\\n\"),\n",
       " (63276, '。\",\\n'),\n",
       " (63953, '监听页面'),\n",
       " (75618, '微软雅黑'),\n",
       " (78900, '。”\\n\\n'),\n",
       " (96332, '”。\\n\\n'),\n",
       " (104248, '的情况下'),\n",
       " (104290, '传奇私服'),\n",
       " (104343, '越来越多'),\n",
       " (104520, '解决方案'),\n",
       " (104524, '经济发展'),\n",
       " (104566, '产业发展'),\n",
       " (104558, '的基础上'),\n",
       " (104702, '另一方面'),\n",
       " (104831, '责任编辑'),\n",
       " (104842, '知识产权'),\n",
       " (104884, '电子商务'),\n",
       " (104889, '什么样的'),\n",
       " (104915, '的过程中'),\n",
       " (104912, '告诉记者'),\n",
       " (104988, '数据显示'),\n",
       " (104986, '一段时间'),\n",
       " (105073, '这个问题'),\n",
       " (105160, '人力资源'),\n",
       " (105154, '贯彻落实'),\n",
       " (105164, '营商环境'),\n",
       " (105201, '第一时间'),\n",
       " (105237, '这个时候'),\n",
       " (105248, '脱贫攻坚'),\n",
       " (105294, '改革开放'),\n",
       " (105283, '没有任何'),\n",
       " (105305, '新开传奇'),\n",
       " (105368, '游戏玩家'),\n",
       " (105366, '一带一路'),\n",
       " (105402, '乡村振兴'),\n",
       " (105441, '经济社会'),\n",
       " (105464, '这是一个'),\n",
       " (105492, '中华人民'),\n",
       " (105508, '国有企业'),\n",
       " (105515, '人民群众'),\n",
       " (105506, '中小企业'),\n",
       " (105532, '与此同时'),\n",
       " (105579, '澳大利亚'),\n",
       " (105745, '没有什么'),\n",
       " (105816, '转型升级'),\n",
       " (105884, '也就是说'),\n",
       " (105889, '体育投注'),\n",
       " (105940, '已经成为'),\n",
       " (105947, '服务中心'),\n",
       " (106044, '中国特色'),\n",
       " (106093, '联系我们'),\n",
       " (106108, '党委书记'),\n",
       " (106107, '金融机构'),\n",
       " (106146, '不仅仅是'),\n",
       " (106213, '领导小组'),\n",
       " (106236, '查看更多'),\n",
       " (106241, '中国经济'),\n",
       " (106250, '每个人都'),\n",
       " (106249, '作为一个'),\n",
       " (106284, '可以说是'),\n",
       " (106286, '党员干部'),\n",
       " (106334, '这种情况'),\n",
       " (106339, '管理人员'),\n",
       " (106351, '企业发展'),\n",
       " (106381, '信息技术'),\n",
       " (106392, '这样一个'),\n",
       " (106439, '这个地图'),\n",
       " (106463, '小微企业'),\n",
       " (106480, '日常生活'),\n",
       " (106543, '有一定的'),\n",
       " (106583, '研究中心'),\n",
       " (106572, '经济效益'),\n",
       " (106746, '中华民族'),\n",
       " (106802, '不一样的'),\n",
       " (106810, '体系建设'),\n",
       " (106841, '扫黑除恶'),\n",
       " (106886, '民营企业'),\n",
       " (106901, '人才培养'),\n",
       " (106923, '改革发展'),\n",
       " (106943, '个百分点'),\n",
       " (106979, '的一部分'),\n",
       " (106993, '金融服务'),\n",
       " (107065, '副总经理'),\n",
       " (107079, '生产经营'),\n",
       " (107087, '中国人民'),\n",
       " (107094, '网络游戏'),\n",
       " (107129, '产品质量'),\n",
       " (107124, '解决问题'),\n",
       " (107127, '很多人都'),\n",
       " (107121, '发展战略'),\n",
       " (107170, '一直以来'),\n",
       " (107274, '委副书记'),\n",
       " (107271, '的可能性'),\n",
       " (107308, '服务平台'),\n",
       " (107325, '今年以来'),\n",
       " (107338, '市场经济'),\n",
       " (107407, '环境保护')]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_chinese_tokens = sorted(chinese_tokens.items(), key=lambda x: len(x[1]), reverse=True)\n",
    "sorted_chinese_tokens[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unstable tokens look for all tokens that contains the current token\n",
    "# this is useful for text completion where the current char might be in the middle of a token\n",
    "# i.e. '你' would have a low probability of generating '你好' because '你' is in the middle of the token\n",
    "# we seek to find all tokens that contain '你' \n",
    "\n",
    "unstable_tokens = {} # str -> Set[tuple(int, str)]\n",
    "for idx, text in chinese_tokens.items():\n",
    "    if len(text) > 1:\n",
    "        for i in range(1, len(text)):\n",
    "            prefix = text[:i]\n",
    "            if prefix not in unstable_tokens:\n",
    "                unstable_tokens[prefix] = set()\n",
    "            unstable_tokens[prefix].add((idx, text))\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(101900, '你们'),\n",
       " (102762, '你会'),\n",
       " (103929, '你的'),\n",
       " (105043, '你是'),\n",
       " (105048, '你可以'),\n",
       " (105182, '你要'),\n",
       " (105365, '你就'),\n",
       " (105665, '你说'),\n",
       " (106878, '你看'),\n",
       " (107409, '你想'),\n",
       " (107411, '你也'),\n",
       " (107719, '你不'),\n",
       " (107733, '你知道'),\n",
       " (107740, '你在'),\n",
       " (107809, '你能'),\n",
       " (108340, '你还'),\n",
       " (108386, '你好'),\n",
       " (109111, '你怎么'),\n",
       " (110043, '你觉得'),\n",
       " (112735, '你需要'),\n",
       " (113254, '你现在'),\n",
       " (113540, '你喜欢'),\n",
       " (115078, '你会发现'),\n",
       " (115337, '你应该'),\n",
       " (117120, '你以为')}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unstable_tokens[\"你\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save unstable tokens as raw binary data\n",
    "import pickle\n",
    "with open(\"unstable_tokens.pkl\", \"wb\") as f:\n",
    "    pickle.dump(unstable_tokens, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(101900, '你们'),\n",
       " (102762, '你会'),\n",
       " (103929, '你的'),\n",
       " (105043, '你是'),\n",
       " (105048, '你可以'),\n",
       " (105182, '你要'),\n",
       " (105365, '你就'),\n",
       " (105665, '你说'),\n",
       " (106878, '你看'),\n",
       " (107409, '你想'),\n",
       " (107411, '你也'),\n",
       " (107719, '你不'),\n",
       " (107733, '你知道'),\n",
       " (107740, '你在'),\n",
       " (107809, '你能'),\n",
       " (108340, '你还'),\n",
       " (108386, '你好'),\n",
       " (109111, '你怎么'),\n",
       " (110043, '你觉得'),\n",
       " (112735, '你需要'),\n",
       " (113254, '你现在'),\n",
       " (113540, '你喜欢'),\n",
       " (115078, '你会发现'),\n",
       " (115337, '你应该'),\n",
       " (117120, '你以为')}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load unstable tokens\n",
    "import pickle\n",
    "with open(\"unstable_tokens.pkl\", \"rb\") as f:\n",
    "    unstable_tokens = pickle.load(f)\n",
    "\n",
    "unstable_tokens[\"你\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save chinese tokens as raw binary data\n",
    "import pickle\n",
    "with open(\"chinese_tokens.pkl\", \"wb\") as f:\n",
    "    pickle.dump(chinese_tokens, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "together",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
